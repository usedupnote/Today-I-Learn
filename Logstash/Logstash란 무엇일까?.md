## `Logstash란 무엇일까?`

로그를 수집하는 쪽에서 로그 형태를 분석하고 시스템에서 인식할 수 있도록 로그를 정제하는 작업이 필요한데, 로그스태시는 이 과정을 쉽고 편하게 할 수 있도로고 지원한다.

로그스태시는 플로그인 기반의 오픈소스 데이터 처리 파이플인 도구이다. 다소 복잡하고 귀찮은 데이터 전처리 과정을 별도의 애플리케이션 작성 없이 비교적 간단한 설정만으로 수행할 수 있다.

<br>

## `파이프라인`

로그스태시의 가장 중요한 부분은 파티프라인이다. 파이프라인은 데이터를 입력받아 실시간으로 변경하고 이를 다른 시스템에 전달하는 역할을 하는 로그스태시의 핵심 기능이다.

파이프라인은 `입력`, `필터`, `출력`이라는 세 가지 구성요소로 이뤄진다. `입력`, `출력`은 필수 구성요소이고, `필터`는 옵션이다. 

```
데이터 소스 -> (입력 -> 필터 -> 출력) -> 엘라스틱 서치
```

- 입력: 소스로부터 데이터를 받아들이는 모듈
- 필터: 입력으로 들어오는 데이터를 원하는 형태로 가공하는 모듈
- 출력: 데이터를 외부로 전달하는 모듈

<br>

```
input {
  { 입력 플러그인 } 
}

filter {
  { 필터 플러그인 } 
}

output {
  { 출력 플러그인 } 
}
```

<br>

## `입력`

[자주 사용하는 입력 플러그인]

- `file`: 리눅스 file -f 명령처럼 파일을 스트리밍하여 이벤트를 읽어옴
- `syslog`: 네트워크를 통해 전달되는 시스로그를 수신
- `kafka`: 카프카의 토픽에서 데이터를 읽어 들인다.
- `jdbc`: JDBC 드라이버로 지정한 일정마다 쿼리를 실행해 결과를 읽어 들인다.

<br>

## `필터`

필터는 비정형 데이터를 정형화하고 데이터 분석을 위한 구조를 잡아준다. 손쉽게 추출하거나 형태를 변환하고 부족한 정보는 추가하는 등 전반적인 데이터 정제/가공 작업을 수행할 수 있다. 

[자주 사용하는 입력 플러그인]

- `grok`: grok 패턴을 사용해 메세지를 구조화된 형태로 분석한다. grok 패턴은 일반적인 정규식과 유사하나, 추가적으로 미리 정의된 패턴이나 필드 이름 설정, 데이터 타입 정의 등을 도와준다.
- `dissect`: 간단한 패턴을 사용해 메세지를 구조화된 형태로 분석한다. 정규식을 사용하지 않아 grok에 비해 자유도는 조금 떨어지지만 더 빠른 처리가 가능하다. 
- `mutate`: 필드명을 변경하거나 문자열 처리 등 일반적인 가공 함수들을 제공한다.
- `date`: 문자열을 지정한 패턴의 날짜형으로 분석한다.

<br>

## `출력`

출력은 파이프라인의 입력과 필터를 거쳐 가공된 데이터를 지정한 대상으로 내보내는 단계이다. 

[자주 사용하는 입력 플러그인]

- `elasticsearch`: 가장 많이 사용되는 출력 플러그인으로, bulk API를 사용해 엘라스틱서치에 인덱싱을 수행한다.
- `file`: 지정한 파일의 새로운 줄에 데이터를 기록한다.
- `kafka`: 카프카 토픽에 데이터를 기록한다.